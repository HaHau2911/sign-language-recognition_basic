import numpy as np
from time import time


class DeepNeuralNetwork():
    def __init__(self, layers, activation='sigmoid'):
        self.layers = layers
        self.t = 1

# ====== chọn Activation function ==============================================================================================================================
        if activation == 'relu':
            self.activation = self.relu
        elif activation == 'sigmoid':
            self.activation = self.sigmoid
        else:
            raise ValueError(
                "Activation function is currently not support, please use 'relu' or 'sigmoid' instead.")

# ====== Khởi tạo weight, bias ================================================================================================================================
        self.params = self.initialize()
# ====== Lưu các giá trị tạm thời của các node lúc lan truyền ngược ===========================================================================================
        self.cache = {}

# == Activation function ======================================================================================================================================
    def relu(self, x, derivative=False):
        if derivative:
            x = np.where(x < 0, 0, x)
            x = np.where(x >= 0, 1, x)
            return x
        return np.maximum(0, x)

    def sigmoid(self, x, derivative=False):
        if derivative:
            return (np.exp(-x))/((np.exp(-x)+1)**2)
        return 1.0/(1.0 + np.exp(-x))

    def softmax(self, x):
        # Numerically stable with large exponentials
        exps = np.exp(x - x.max())
        return exps / np.sum(exps, axis=0)

# == Hàm khởi tạo weight, bias =================================================================================================================================
    def initialize(self):
        params = {}
        for i in range(1, len(self.layers)):
            hidden_1 = self.layers[i-1]
            hidden_2 = self.layers[i]
            params.update(
                {f'W{i}': np.random.randn(hidden_2, hidden_1) * np.sqrt(1./hidden_1)})
            params.update(
                {f'b{i}': np.zeros((hidden_2, 1)) * np.sqrt(1./hidden_1)})
        return params

# == Hàm khởi tạo optimizer ====================================================================================================================================
    def initialize_momemtum_optimizer(self):
        momemtum_opt = {}
        for i in range(1, len(self.layers)):
            momemtum_opt.update(
                {f'W{i}': np.zeros(self.params[f'W{i}'].shape)})
            momemtum_opt.update(
                {f'b{i}': np.zeros(self.params[f'b{i}'].shape)})
        return momemtum_opt

    def initialize_adam_optimizer(self):
        adam_opt = {}
        for i in range(1, len(self.layers)):
            adam_opt.update(
                {f'mW{i}': np.zeros(self.params[f'W{i}'].shape)})
            adam_opt.update(
                {f'mb{i}': np.zeros(self.params[f'b{i}'].shape)})
            adam_opt.update(
                {f'vW{i}': np.zeros(self.params[f'W{i}'].shape)})
            adam_opt.update(
                {f'vb{i}': np.zeros(self.params[f'b{i}'].shape)})
        return adam_opt

# == Hàm lan truyền thẳng =====================================================================================================================================
    def feed_forward(self, x):
        self.cache['A0'] = x
        for i in range(1, len(self.layers)):
            if i == 1:
                self.cache[f'Z{i}'] = np.matmul(
                    self.params[f'W{i}'], self.cache[f'A{i-1}'].T) + self.params[f'b{i}']
            else:
                self.cache[f'Z{i}'] = np.matmul(
                    self.params[f'W{i}'], self.cache[f'A{i-1}']) + self.params[f'b{i}']

            if i == len(self.layers)-1:
                self.cache[f'A{i}'] = self.softmax(self.cache[f'Z{i}'])
            else:
                self.cache[f'A{i}'] = self.activation(self.cache[f'Z{i}'])

        return self.cache[f'A{len(self.layers)-1}']

# == Hàm lan truyền ngược =====================================================================================================================================
    def back_propagate(self, y, output):
        current_batch_size = y.shape[0]
        self.grads = {}
        for i in range(len(self.layers)-1, 0, -1):
            if i == len(self.layers)-1:
                dZ = output - y.T
                dW = (1./current_batch_size) * \
                    np.matmul(dZ, self.cache[f'A{i-1}'].T)
            elif i == 1:
                dA = np.matmul(self.params[f'W{i+1}'].T, dZ)
                dZ = dA * self.activation(self.cache[f'Z{i}'], derivative=True)
                dW = (1./current_batch_size) * \
                    np.matmul(dZ, self.cache[f'A{i-1}'])
            else:
                dA = np.matmul(self.params[f'W{i+1}'].T, dZ)
                dZ = dA * self.activation(self.cache[f'Z{i}'], derivative=True)
                dW = (1./current_batch_size) * \
                    np.matmul(dZ, self.cache[f'A{i-1}'].T)

            db = (1./current_batch_size) * np.sum(dZ, axis=1, keepdims=True)
            self.grads.update({f'W{i}': dW})
            self.grads.update({f'b{i}': db})
        return self.grads

# == Loss function ============================================================================================================================================
    def cross_entropy_loss(self, y, output):
        n = y.shape[0]
        # l_sum = np.sum(np.multiply(y.T, np.log(output)))
        # m = y.shape[0]
        # l = -(1./m) * l_sum
        return -np.sum(y.T * np.log(output))/n

# == Cập nhật weight, bias theo optimize ======================================================================================================================
    def optimize(self, l_rate=0.1, beta1=.9, beta2=.9999, epsilon=1e-8):
        if self.optimizer == "sgd":
            for key in self.params:
                self.params[key] = self.params[key] - l_rate * self.grads[key]
        elif self.optimizer == "momentum":
            for key in self.params:
                self.momemtum_opt[key] = (
                    beta1 * self.momemtum_opt[key] + (1. - beta1) * self.grads[key])
                self.params[key] = self.params[key] - \
                    l_rate * self.momemtum_opt[key]
        elif self.optimizer == "adam":
            for key in self.params:
                self.adam_opt[f'm{key}'] = (beta1 *
                                            self.adam_opt[f'm{key}'] + (1-beta1) * self.grads[key])
                self.adam_opt[f'v{key}'] = (beta2 *
                                            self.adam_opt[f'v{key}'] + (1-beta2) * self.grads[key]**2)
                self.params[key] -= l_rate*(self.adam_opt[f'm{key}']/(1-beta1**self.t)) / (
                    np.sqrt(self.adam_opt[f'v{key}']/(1-beta2**self.t))+epsilon)
        else:
            raise ValueError(
                "Optimizer is currently not support, please use 'sgd' or 'momentum' or 'adam' instead.")

# == Accuracy function ========================================================================================================================================
    def accuracy(self, y, output):
        return np.mean(np.argmax(y, axis=-1) == np.argmax(output.T, axis=-1))

# == Train model ==============================================================================================================================================
    def fit(self, x_train, y_train, x_test, y_test, epochs=10,
            batch_size=64, optimizer='momentum', l_rate=0.1, beta1=.9, beta2=.9999, epsilon=1e-8):
        # Hyperparameters
        self.epochs = epochs
        self.batch_size = batch_size
        num_batches = -(-x_train.shape[0] // self.batch_size)

        # khởi tạo các tham số optimizer
        self.optimizer = optimizer
        if self.optimizer == 'momentum':
            self.momemtum_opt = self.initialize_momemtum_optimizer()
        elif self.optimizer == 'adam':
            self.momemtum_opt = self.initialize_momemtum_optimizer()
            self.adam_opt = self.initialize_adam_optimizer()

        _a, _l, _va, _vl = [], [], [], []
        for i in range(self.epochs):
            # trộn dữ liệu
            permutation = np.random.permutation(x_train.shape[0])
            x_train_shuffled = x_train[permutation]
            y_train_shuffled = y_train[permutation]

            for j in range(num_batches):
                # bỏ vào batch số dữ liệu để train
                begin = j * self.batch_size
                end = min(begin + self.batch_size, x_train.shape[0]-1)
                x = x_train_shuffled[begin:end]
                y = y_train_shuffled[begin:end]

                # lan truyền thẳng
                output = self.feed_forward(x)
                # lan truyền ngược
                grad = self.back_propagate(y, output)
                # update weight, bias
                self.optimize(l_rate=l_rate, beta1=beta1,
                              beta2=beta2, epsilon=epsilon)
                # cộng thêm 1 vào t cho hàm adam
                self.t += 1

            # Tính acc, loss
            output = self.feed_forward(x_train)
            train_acc = self.accuracy(y_train, output)
            train_loss = self.cross_entropy_loss(y_train, output)
            _a.append(train_acc)
            _l.append(train_loss)
            # Tính val_acc, val_loss
            output = self.feed_forward(x_test)
            test_acc = self.accuracy(y_test, output)
            test_loss = self.cross_entropy_loss(y_test, output)
            _va.append(test_acc)
            _vl.append(test_loss)

            template = "Epoch {:4d}: acc = {:.4f} | loss = {:.4f} | val_acc = {:.4f} | val_loss = {:.4f}"
            print(template.format(i+1, train_acc, train_loss, test_acc, test_loss))
        return {'acc': _a, 'loss': _l, 'val_acc': _va, 'val_loss': _vl}

# == hàm predict ==============================================================================================================================================
    def predict(self, x):
        x = x.reshape(-1, x.shape[0])
        pred = self.feed_forward(x)
        return pred
